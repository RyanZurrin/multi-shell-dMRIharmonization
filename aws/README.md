# MRI Data Harmonization Scripts

This repository contains a set of scripts designed to streamline the process of MRI data harmonization. The scripts are as follows:

1. `download_from_s3.py`: A Python script to download MRI data files from an AWS S3 bucket using multithreading for improved performance.

2. `write_local_paths.py`: A Python script to generate a text file with the local paths of the downloaded .nii.gz and .nii.gz mask files, required for subsequent harmonization steps.

3. `run_template.sh`: A Bash script to initiate the harmonization process. This script uses the local paths generated by `write_local_paths.py` to access and process the MRI data.

4. `automate_pipeline.py`: A Python script to automate the full process. It reads the necessary parameters from a configuration file and sequentially runs the above three scripts.

These scripts are tailored specifically for MRI data but can be adapted for use with other types of data.

## Table of Contents

1. [Requirements](#requirements)
2. [Usage](#usage)
3. [Input File Format](#input-file-format)
4. [Creating a Local Paths Text File](#creating-a-local-paths-text-file)
5. [Running the Template Script](#running-the-template-script)
6. [Automating the Process](#automating-the-process)
7. [Contributing](#contributing)
8. [License](#license)

## Requirements

- Python 3.7 or newer
- Boto3 library
- s3fs library

You can install the required Python libraries using pip:

```sh
pip install boto3 s3fs
```

## Usage

To use this script, you'll need to provide a text file containing the S3 paths to the files you want to download, and specify the local directory where the files should be downloaded to.

Here's an example of how to run the script:

```sh
python download_from_s3.py --textfile <path-to-your-textfile> --directory <path-to-local-directory>
```

Replace `<path-to-your-textfile>` with the path to your text file, and `<path-to-local-directory>` with the path to the local directory where you want the files to be downloaded to.

## Input File Format

The input text file should contain two S3 paths per line, separated by a comma. The first path should point to a .nii.gz file, and the second path should point to a .nii.gz mask file.

Here's an example of what the input file format looks like:

```angular2html
s3://mybucket/path/to/file1.nii.gz,s3://mybucket/path/to/file1_mask.nii.gz
s3://mybucket/path/to/file2.nii.gz,s3://mybucket/path/to/file2_mask.nii.gz
...
```

For each line, the script will download the .nii.gz and .nii.gz mask file, as well as any .bval and .bvec files that are in the same directory.

## Creating a Local Paths Text File

Once the files are downloaded, you can use the following script to create a new text file that contains the local paths to the .nii.gz and .nii.gz mask files:

```sh
python write_local_paths.py --directory <path-to-root-directory> --output <path-to-output-textfile>
```

Replace `<path-to-root-directory>` with the path to the root directory that contains the downloaded files, and `<path-to-output-textfile>` with the path to the text file where you want the local paths to be written to.

## Running the Template Script

After downloading the necessary files and generating the local paths files, you can run the `run_template.sh` script to perform further analysis. This script accepts several command-line arguments to specify paths and processing options. Here's an example of how to run the script:

```sh
./run_template.sh -r /path/to/reference-list.txt -t /path/to/target-list.txt -n ReferenceName -T TargetName -p /path/to/template/directory -d 4
```

Please ensure to replace `/path/to/reference-list.txt`, `/path/to/target-list.txt`, `ReferenceName`, `TargetName`, and `/path/to/template/directory` with the actual paths and names on your local machine. The `-d` flag is used to specify the number of processes or threads to use (use -1 for all available), with the default being 4.


## Automating the Process

To simplify the harmonization process, you can use the `automate_template_pipeline.py` script which automates the full process.

The automated pipeline requires a configuration file in INI format that specifies the necessary parameters for each script. Here's an example of how the configuration file (`config.ini`) might look:

```ini
[s3_download]
textfile = path/to/your/textfile
directory = path/to/local/directory
multithreaded = number_of_threads

[local_paths]
directory = path/to/root/directory
output = path/to/output/textfile

[bash_script]
ref_list = path/to/reference-list.txt
tar_list = path/to/target-list.txt
ref_name = ReferenceName
tar_name = TargetName
template = path/to/template/directory
nproc = number_of_threads
```
Replace `path/to/your/textfile`, `path/to/local/directory`, `number_of_threads`, `path/to/root/directory`, `path/to/output/textfile`, `path/to/reference-list.txt`, `path/to/target-list.txt`, `ReferenceName`, `TargetName`, `path/to/template/directory`, and `number_of_threads` with your actual values.

Once the configuration file is set, you can run the automated pipeline with the following command:
```sh
python automate_template_pipeline.py --config config.ini
``
```



## Contributing

If you have suggestions for how this script could be improved, please fork this repository and create a pull request, or simply open an issue with the tag "enhancement". Thank you!

## License

This project is licensed under the terms of the MIT license.