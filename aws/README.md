![](../doc/pnl-bwh-hms.png)

[![DOI](https://zenodo.org/badge/doi/10.5281/zenodo.3451427.svg)](https://doi.org/10.5281/zenodo.3451427) [![Python](https://img.shields.io/badge/Python-3.7-green.svg)]() [![Platform](https://img.shields.io/badge/Platform-linux--64%20%7C%20osx--64-orange.svg)]()

# Harmonization of Multi-shell Diffusion MRI Data

This repository contains scripts and code that form a part of a harmonization pipeline. This pipeline is aimed at harmonizing multi-shell Diffusion MRI (dMRI) data across various sites and scanner protocols. The original code was primarily developed by Tashrif Billah and Yogesh Rathi at the Brigham and Women's Hospital (Harvard Medical School).

## Additional AWS-focused Scripts 
The following scritps have been developed by [Ryan Zurrin](mailto:rzurrin@bwh.harvard.edu)

1. `download_from_s3.py`: A Python script to download MRI data files from an AWS S3 bucket using multithreading for improved performance. This script needs to be run twice, once for the reference files and once for the target files.

2. `write_local_paths.py`: A Python script to generate a text file with the local paths of the downloaded .nii.gz and .nii.gz mask files, required for subsequent harmonization steps. Similar to the `download_from_s3.py` script, this script also needs to be run twice.

3. `run_template.sh`: A Bash script to initiate the harmonization process. This script uses the local paths generated by write_local_paths.py to access and process the MRI data.

4. `automate_harmonization_pipeline.py`: A Python script to automate the full process. It reads the necessary parameters from a configuration file and sequentially runs the above three scripts.


## Table of Contents

1. [Requirements](#requirements)
2. [Usage](#usage)
3. [Input File Format](#input-file-format)
4. [Creating a Local Paths Text File](#creating-a-local-paths-text-file)
5. [Running the Template Script](#running-the-template-script)
6. [Automating the Process](#automating-the-process)
7. [Contributing](#contributing)
8. [License](#license)

## Requirements

Before proceeding with the AWS-focused scripts, please ensure that you have followed and completed the setup and requirements in the root [README](../README.md) of this repository. Once you have done so, you may continue with the following requirements specific to these scripts.

This project requires:

- Python 3.7 or newer
- s3fs library

These Python libraries can be installed using pip if they are not already installed:

```sh
pip install s3fs
```

Please make sure that all these requirements are properly installed and configured before proceeding with the scripts in the 'aws' directory. This will ensure that the pipeline runs smoothly without any disruptions.

## Usage


To use the `download_from_s3.py` script, you'll need to provide a text file containing the S3 paths to the files you want to download, specify the local directory where the files should be downloaded to, and specify the number of threads to be used for multithreading.

Here's an example of how to run the script:

```sh
python download_from_s3.py --textfile <path-to-your-textfile> --directory <path-to-local-directory> --threads <number_of_threads>
```

Replace `<path-to-your-textfile>` with the path to your text file, `<path-to-local-directory>` with the path to the local directory where you want the files to be downloaded to, and `<number_of_threads>` with the number of threads you want to use for multithreading.

Remember to run this script twice, once for the reference files and once for the target files.

## Input File Format

The input text file should contain two S3 paths per line, separated by a comma. The first path should point to a .nii.gz file, and the second path should point to a .nii.gz mask file.

Here's an example of what the input file format looks like:

```angular2html
s3://mybucket/path/to/file1.nii.gz,s3://mybucket/path/to/file1_mask.nii.gz
s3://mybucket/path/to/file2.nii.gz,s3://mybucket/path/to/file2_mask.nii.gz
...
```

For each line, the script will download the .nii.gz and .nii.gz mask file, as well as any .bval and .bvec files that are in the same directory.

## Creating a Local Paths Text File

Once the files are downloaded, you can use the following script to create a new text file that contains the local paths to the .nii.gz and .nii.gz mask files:

```sh
python write_local_paths.py --directory <path-to-root-directory> --output <path-to-output-textfile>
```

Replace `<path-to-root-directory>` with the path to the root directory that contains the downloaded files, and `<path-to-output-textfile>` with the path to the text file where you want the local paths to be written to.

## Running the Template Script

After downloading the necessary files and generating the local paths files, you can run the `run_template.sh` script to perform further analysis. This script accepts several command-line arguments to specify paths and processing options. Here's an example of how to run the script:

```sh
./run_template.sh -r /path/to/reference-list.txt -t /path/to/target-list.txt -n ReferenceName -T TargetName -p /path/to/template/directory -d 4
```

Please ensure to replace `/path/to/reference-list.txt`, `/path/to/target-list.txt`, `ReferenceName`, `TargetName`, and `/path/to/template/directory` with the actual paths and names on your local machine. The `-d` flag is used to specify the number of processes or threads to use (use -1 for all available), with the default being 4.


## Automating the Process

To simplify the harmonization process, you can use the `automate_harmonization_pipeline.py` script which automates the full process.

The automated pipeline requires a configuration file in INI format that specifies the necessary parameters for each script. Here's an example of how the configuration file (`config.ini`) might look:

```ini
[s3_download]
reference_textfile = /path/to/reference/textfile.txt
target_textfile = /path/to/target/textfile.txt
reference_directory = /path/to/reference/directory
target_directory = /path/to/target/directory
template_path = /path/to/template
multithreading = num_threads

[local_paths]
reference_output = /path/to/reference/output.txt
target_output = /path/to/target/output.txt

[bash_script]
ref_list = /path/to/reference/list.txt
tar_list = /path/to/target/list.txt
ref_name = reference_name
tar_name = target_name
template = /path/to/template
nproc = num_processors
create = True 
process = True
debug = True
```
Please replace the placeholders with your actual values:

- `/path/to/reference/textfile.txt`: Replace with the path to your reference text file.
- `/path/to/target/textfile.txt`: Replace with the path to your target text file.
- `/path/to/reference/directory`: Replace with the path where your reference directory will be downloaded to.
- `/path/to/target/directory`: Replace with the path where your target directory will be downloaded to.
- `/path/to/template`: Replace with the path to your template.
- `num_threads`: Replace with the number of threads you wish to allocate for multithreading.
- `/path/to/reference/output.txt`: Replace with the path to your reference output text file.
- `/path/to/target/output.txt`: Replace with the path to your target output text file.
- `/path/to/reference/list.txt`: Replace with the path to your reference list text file.
- `/path/to/target/list.txt`: Replace with the path to your target list text file.
- `reference_name`: Replace with the name of your reference.
- `target_name`: Replace with the name of your target.
- `/path/to/template`: Replace with the path to your template.
- `num_processors`: Replace with the number of processes you wish to allocate for processing.
- `True`: Replace with `True` if you are creating a template, else replace with `False`.
- `True`: Replace with `True` if you are harmonizing, else replace with `False`.
- `True`: Replace with `True` if you are debugging, else replace with `False`. Must be `False` if `process` is `False`.


Remember to save your configuration file with the `.ini` extension.




Once you have configured your settings, you can run the pipeline using the command line. Here are the available options:

- `--config`: Path to the configuration file.
- `--log_file`: Path to the log file. Default is `pipeline.log` in the same directory as the script.
- `--verbose`: Adding this flag will set verbose to `True`. Default is `False`
- `--create`: Set to `True` to create the template. Default is `True`.
- `--process`: Set to `True` to harmonize the data. Default is `True`.
- `--debug`: Set to `True` to run in debug mode. Default is `False`. Must be `False` if `process` is `False`.

The --create, --process, and --debug flags are set in the configuration file, so you do not need to specify them when running the script. Setting them in the command line will override the values in the configuration file.

Here's an example of how to run the script:
```sh
python automate_template_pipeline.py --config /path/to/config.ini --log_file /path/to/logfile.log --verbose
```


The config file can be named anything, and be located anywhere, as long as you specify the path to the config file when running the script. default is `config.ini` in the same directory as the script.

The automated pipeline also logs important events to a log file named pipeline.log. This log file contains both successful events and any errors that might occur.

Replace the placeholders in the `config.ini` example with your actual paths and values. Make sure that the `config.ini` file is located in the same directory as the `automate_template_pipeline.py` script.


## Contributing

If you have suggestions for how this script could be improved, please fork this repository and create a pull request, or simply open an issue with the tag "enhancement". Thank you!

## License

This project is licensed under the terms of the MIT license.