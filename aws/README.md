![](../doc/pnl-bwh-hms.png)

[![DOI](https://zenodo.org/badge/doi/10.5281/zenodo.3451427.svg)](https://doi.org/10.5281/zenodo.3451427) [![Python](https://img.shields.io/badge/Python-3.6-green.svg)]() [![Platform](https://img.shields.io/badge/Platform-linux--64%20%7C%20osx--64-orange.svg)]()

# Harmonization of Multi-shell Diffusion MRI Data

This repository contains scripts and code that form a part of a harmonization pipeline. This pipeline is aimed at harmonizing multi-shell Diffusion MRI (dMRI) data across various sites and scanner protocols. The original code was primarily developed by Tashrif Billah and Yogesh Rathi at the Brigham and Women's Hospital (Harvard Medical School).

## Additional AWS-focused Scripts 
The following scritps have been developed by [Ryan Zurrin](mailto:rzurrin@bwh.harvard.edu)

1. `download_from_s3.py`: A Python script to download MRI data files from an AWS S3 bucket using multithreading for improved performance.

2. `write_local_paths.py`: A Python script to generate a text file with the local paths of the downloaded .nii.gz and .nii.gz mask files, required for subsequent harmonization steps.

3. `run_template.sh`: A Bash script to initiate the harmonization process. This script uses the local paths generated by write_local_paths.py to access and process the MRI data.

4. `automate_template_pipeline.py`: A Python script to automate the full process. It reads the necessary parameters from a configuration file and sequentially runs the above three scripts.

These scripts are tailored specifically for MRI data but can be adapted for use with other types of data.

## Table of Contents

1. [Requirements](#requirements)
2. [Usage](#usage)
3. [Input File Format](#input-file-format)
4. [Creating a Local Paths Text File](#creating-a-local-paths-text-file)
5. [Running the Template Script](#running-the-template-script)
6. [Automating the Process](#automating-the-process)
7. [Contributing](#contributing)
8. [License](#license)

## Requirements

Before proceeding with the AWS-focused scripts, please ensure that you have followed and completed the setup and requirements in the root [README](../README.md) of this repository. Once you have done so, you may continue with the following requirements specific to these scripts.

This project requires:

- Python 3.7 or newer
- s3fs library

These Python libraries can be installed using pip:

```sh
pip install s3fs
```

Please make sure that all these requirements are properly installed and configured before proceeding with the scripts in the 'aws' directory. This will ensure that the pipeline runs smoothly without any disruptions.

## Usage

To use this script, you'll need to provide a text file containing the S3 paths to the files you want to download, and specify the local directory where the files should be downloaded to.

Here's an example of how to run the script:

```sh
python download_from_s3.py --textfile <path-to-your-textfile> --directory <path-to-local-directory>
```

Replace `<path-to-your-textfile>` with the path to your text file, and `<path-to-local-directory>` with the path to the local directory where you want the files to be downloaded to.

## Input File Format

The input text file should contain two S3 paths per line, separated by a comma. The first path should point to a .nii.gz file, and the second path should point to a .nii.gz mask file.

Here's an example of what the input file format looks like:

```angular2html
s3://mybucket/path/to/file1.nii.gz,s3://mybucket/path/to/file1_mask.nii.gz
s3://mybucket/path/to/file2.nii.gz,s3://mybucket/path/to/file2_mask.nii.gz
...
```

For each line, the script will download the .nii.gz and .nii.gz mask file, as well as any .bval and .bvec files that are in the same directory.

## Creating a Local Paths Text File

Once the files are downloaded, you can use the following script to create a new text file that contains the local paths to the .nii.gz and .nii.gz mask files:

```sh
python write_local_paths.py --directory <path-to-root-directory> --output <path-to-output-textfile>
```

Replace `<path-to-root-directory>` with the path to the root directory that contains the downloaded files, and `<path-to-output-textfile>` with the path to the text file where you want the local paths to be written to.

## Running the Template Script

After downloading the necessary files and generating the local paths files, you can run the `run_template.sh` script to perform further analysis. This script accepts several command-line arguments to specify paths and processing options. Here's an example of how to run the script:

```sh
./run_template.sh -r /path/to/reference-list.txt -t /path/to/target-list.txt -n ReferenceName -T TargetName -p /path/to/template/directory -d 4
```

Please ensure to replace `/path/to/reference-list.txt`, `/path/to/target-list.txt`, `ReferenceName`, `TargetName`, and `/path/to/template/directory` with the actual paths and names on your local machine. The `-d` flag is used to specify the number of processes or threads to use (use -1 for all available), with the default being 4.


## Automating the Process

To simplify the harmonization process, you can use the `automate_template_pipeline.py` script which automates the full process.

The automated pipeline requires a configuration file in INI format that specifies the necessary parameters for each script. Here's an example of how the configuration file (`config.ini`) might look:

```ini
[s3_download]
reference_textfile = /path/to/ref_textfile.txt
target_textfile = /path/to/tar_textfile.txt
reference_directory = /path/to/download/ref_directory
target_directory = /path/to/download/tar_directory
multithreading = 8

[local_paths]
reference_output = /path/to/ref_output.txt
target_output = /path/to/tar_output.txt

[bash_script]
ref_list = /path/to/ref_output.txt
tar_list = /path/to/tar_output.txt
ref_name = ref_name
tar_name = tar_name
template = /path/to/template
nproc = 4
```
Please replace the placeholders with your actual values:

- `/your/path/to/ref_textfile.txt`: Replace with the path to your reference text file.
- `/your/path/to/tar_textfile.txt`: Replace with the path to your target text file.
- `/your/path/to/download/ref_directory`: Replace with the path where your reference directory will be downloaded to.
- `/your/path/to/download/tar_directory`: Replace with the path where your target directory will be downloaded to.
- `number_of_threads`: Replace with the number of threads you wish to allocate for multithreading.
- `/your/path/to/ref_output.txt`: Replace with the path to your reference output text file.
- `/your/path/to/tar_output.txt`: Replace with the path to your target output text file.
- `ReferenceName`: Replace with the name of your reference.
- `TargetName`: Replace with the name of your target.
- `/your/path/to/template/directory`: Replace with the path to your template directory.

Remember to save your configuration file with the `.ini` extension.

Once the configuration file is set, you can run the automated pipeline with the following command:
```sh
python automate_template_pipeline.py --config config.ini
```

The automated pipeline also logs important events to a log file named pipeline.log. This log file contains both successful events and any errors that might occur.

Replace the placeholders in the `config.ini` example with your actual paths and values. Make sure that the `config.ini` file is located in the same directory as the `automate_template_pipeline.py` script.


## Contributing

If you have suggestions for how this script could be improved, please fork this repository and create a pull request, or simply open an issue with the tag "enhancement". Thank you!

## License

This project is licensed under the terms of the MIT license.