# Harmonization of Multi-shell Diffusion MRI Data

## Adapted Multi-shell Diffusion MRI Harmonization Pipeline for AWS

![PNL-BWH-HMS Logo](docs/pnl-bwh-hms.png "PNL-BWH-HMS Logo")

[![DOI](https://zenodo.org/badge/doi/10.5281/zenodo.3451427.svg)](https://doi.org/10.5281/zenodo.3451427) [![Python](https://img.shields.io/badge/Python-3.7-green.svg)](#prerequisites) [![Platform](https://img.shields.io/badge/Platform-linux--64%20%7C%20osx--64-orange.svg)](#prerequisites)

This repository contains scripts and code that form a part of a harmonization pipeline. This pipeline is aimed at harmonizing multi-shell Diffusion MRI (dMRI) data across various sites and scanner protocols. The original code was primarily developed by Tashrif Billah and Yogesh Rathi at the Brigham and Women's Hospital (Harvard Medical School).

## Table of Contents

* [Additional AWS-focused Scripts and Adapted Pipeline Overview](#additional-aws-focused-scripts-and-adapted-pipeline-overview)
* [Prerequisites](#prerequisites)
* [Usage](#usage)
  * [Running the Scripts Separately](#running-the-scripts-separately)
    * [Downloading the Files](#downloading-the-files)
    * [Input File Format](#input-file-format)
    * [Creating a Local Paths Text File](#creating-a-local-paths-text-file)
    * [Running the Template Script](#running-the-template-script)
  * [Automating the Harmonization Process](#automating-the-harmonization-process)
    * [Configuration File (`config.ini`)](#configuration-file-configini)
      * [Configuration Parameters](#configuration-parameters)
    * [Running the Automated Pipeline](#running-the-automated-pipeline)
      * [Command-line Options](#command-line-options)
        * [Running Directly](#running-directly)
        * [Running in the Background](#running-in-the-background)
        * [Using `screen` for Detached Execution](#using-screen-for-detached-execution)
* [Contributing](#contributing)
* [License](#license)

## Additional AWS-focused Scripts and Adapted Pipeline Overview

### Overview

The original pipeline was designed to work with local directories. This adaptation of the pipeline extends the original pipeline to work with AWS S3 buckets. The following scripts have been developed to automate the process of downloading the necessary files from an AWS S3 bucket, generating the local paths text files, and running the template script to perform the harmonization process. The scripts are described in detail below.

### AWS-focused Scripts

The following Python scripts have been developed by [Ryan Zurrin](mailto:rzurrin@bwh.harvard.edu) and are used to extend the original pipeline to work with AWS S3 buckets. These scripts are located in the `aws` directory of the `hcp_aws` branch of this repository.

1. `download_from_s3.py`: A Python script to download MRI data files from an AWS S3 bucket using multithreading for improved performance. This script needs to be run twice, once for the reference files and once for the target files.

2. `write_local_paths.py`: A Python script to generate a text file with the local paths of the downloaded .nii.gz and .nii.gz mask files, required for subsequent harmonization steps. Similar to the `download_from_s3.py` script, this script also needs to be run twice.

3. `run_template.sh`: A Bash script to initiate the harmonization process. This script uses the local paths generated by write_local_paths.py to access and process the MRI data.

4. `automate_harmonization_pipeline.py`: A Python script to automate the full process. It reads the necessary parameters from a configuration file and sequentially runs the above three scripts.

## Prerequisites

### Initial environment setup option 1: Using the prebuilt AMI

The adapted pipeline uses a memory optimized EC2 instance, `r5d.4xlarge` which should be used with the prebuilt AMI `Harmonization_v9.27.2023`.

### Initial environment setup option 2: Building the environment from scratch

If you are not using the prebuilt AMI, you will need to build the environment from scratch.
    1. create a `r5d.4xlarge` EC2 instance with any Linux distribution:
    2. Follow and complete the setup and requirements from the original repo's instructions found in this [README](docs/README.md) file.
    3. Setup the AWS CLI and configure any instance IAM roles or permissions needed to access the S3 bucket.
    4. Install additional Python libraries:
       - [s3fs](https://s3fs.readthedocs.io/en/latest/install.html)
       - [tqdm](https://tqdm.github.io/)

These Python libraries can be installed using pip if they are not already installed:

```sh
pip install s3fs
pip install tqdm
```

Please make sure that all these requirements are properly installed and configured before proceeding with the scripts in the 'aws' directory. This will ensure that the pipeline runs smoothly without any disruptions.

## Usage

The adapted pipeline can be ran by running each of the parts separately, or by using the `automate_harmonization_pipeline.py` script to automate the full process.

### Running the Scripts Separately

#### Downloading the Files

To use the `download_from_s3.py` script, you'll need to provide a text file containing the S3 paths to the files you want to download, specify the local directory where the files should be downloaded to, and specify the number of threads to be used for multithreading.

Here's an example of how to run the script:

```sh
python download_from_s3.py --textfile <path-to-your-textfile> --directory <path-to-local-directory> --threads <number_of_threads>
```

Replace `<path-to-your-textfile>` with the path to your text file, `<path-to-local-directory>` with the path to the local directory where you want the files to be downloaded to, and `<number_of_threads>` with the number of threads you want to use for multithreading.

Remember to run this script twice, once for the reference files and once for the target files.

#### Input File Format

The input text file should contain two S3 paths per line, separated by a comma. The first path should point to a .nii.gz file, and the second path should point to a .nii.gz mask file.

Here's an example of what the input file format looks like:

```angular2html
s3://mybucket/path/to/file1.nii.gz,s3://mybucket/path/to/file1_mask.nii.gz
s3://mybucket/path/to/file2.nii.gz,s3://mybucket/path/to/file2_mask.nii.gz
...
```

For each line, the script will download the .nii.gz and .nii.gz mask file, as well as any .bval and .bvec files that are in the same directory.

#### Creating a Local Paths Text File

Once the files are downloaded, you can use the following script to create a new text file that contains the local paths to the .nii.gz and .nii.gz mask files:

```sh
python write_local_paths.py --directory <path-to-root-directory> --output <path-to-output-textfile>
```

Replace `<path-to-root-directory>` with the path to the root directory that contains the downloaded files, and `<path-to-output-textfile>` with the path to the text file where you want the local paths to be written to.

#### Running the Template Script

After downloading the necessary files and generating the local paths files, you can run the `run_template.sh` script to perform further analysis. This script accepts several command-line arguments to specify paths and processing options. Here's an example of how to run the script:

```sh
./run_template.sh -r /path/to/reference-list.txt -t /path/to/target-list.txt -n ReferenceName -T TargetName -p /path/to/template/directory -d 4
```

Please ensure to replace `/path/to/reference-list.txt`, `/path/to/target-list.txt`, `ReferenceName`, `TargetName`, and `/path/to/template/directory` with the actual paths and names on your local machine. The `-d` flag is used to specify the number of processes or threads to use (use -1 for all available), with the default being 4.

#### Running the Harmonization Script

After the template script has finished running, you can run the `run_harmonization.sh` script to perform the harmonization process. This script accepts several command-line arguments to specify paths and processing options. Here's an example of how to run the script:

```sh
./run_harmonization.sh -t /path/to/target-list.txt -n ReferenceName -T TargetName -p /path/to/template/directory -d 4
```

Please ensure to replace `/path/to/target-list.txt`, `ReferenceName`, `TargetName`, and `/path/to/template/directory` with the actual paths and names on your local machine. The `-d` flag is used to specify the number of processes or threads to use (use -1 for all available), with the default being 4.

### Automating the Harmonization Process

The `automate_harmonization_pipeline.py` script provides a way to automate the entire harmonization process. This script requires a configuration file in INI format to specify the parameters for each step of the pipeline.

#### Configuration File (`config.ini`)

Here's a sample configuration file layout:

```ini
[s3_download]
reference_textfile = /path/to/reference/textfile.txt
target_textfile = /path/to/target/textfile.txt
reference_directory = /path/to/reference/directory
target_directory = /path/to/target/directory
template_path = /path/to/template
multithreading = num_threads

[local_paths]
reference_output = /path/to/reference/output.txt
target_output = /path/to/target/output.txt

[bash_script]
ref_list = /path/to/reference/list.txt
tar_list = /path/to/target/list.txt
ref_name = reference_name
tar_name = target_name
template = /path/to/template
nproc = num_processors
create = True
process = True
debug = True
```

##### Configuration Parameters

* `reference_textfile`: Path to the text file containing S3 paths for reference files.
* `target_textfile`: Path to the text file containing S3 paths for target files.
* `reference_directory`: Local directory where reference files will be downloaded.
* `target_directory`: Local directory where target files will be downloaded.
* `template_path`: Path to the template used for harmonization.
* `num_threads`: Number of threads for multithreading.
* `reference_output`: Output text file containing local paths for reference files.
* `target_output`: Output text file containing local paths for target files.
* `ref_list`: Text file containing a list of reference files.
* `tar_list`: Text file containing a list of target files.
* `ref_name`: Name to identify the reference dataset.
* `tar_name`: Name to identify the target dataset.
* `template`: Path to the template used for harmonization.
* `num_processors`: Number of processors to use.
* `create`: Set to `True` if creating a template, otherwise `False`.
* `process`: Set to `True` if processing, otherwise `False`.
* `debug`: Set to `True` for debug mode, otherwise `False`.

The config file can be named anything, and be located anywhere, as long as you specify the path to the config file when running the script. default is `config.ini` in the same directory as the script.

The automated pipeline also logs important events to a log file named pipeline.log. This log file contains both successful events and any errors that might occur.

Replace the placeholders in the `config.ini` example with your actual paths and values. Make sure that the `config.ini` file is located in the same directory as the `automate_template_pipeline.py` script or specify the path to the config file when running the script.

##### Command-line Options

* `--config`: Specifies the path to the configuration file.
* `--log_file`: Specifies the path to the log file (default is `pipeline.log`).
* `--verbose`: Enables verbose output (default is `False`).
* `--create`: Overrides the `create` setting in the configuration file.
* `--process`: Overrides the `process` setting in the configuration file.
* `--debug`: Overrides the `debug` setting in the configuration file.

Note: The `--create`, `--process`, and `--debug` command-line flags will override their corresponding settings in the `config.ini` file if provided.

Note: The `--verbose` flag is good to set to allow for detailed logging to be output to the terminal as well as the log file. Or you can monitory the log file with `tail -f pipeline.log` in a separate terminal window.

#### Running the Automated Pipeline

Once the `config.ini` file is set up, you can run the automated pipeline in several ways:

##### Running Directly

If you anticipate that the process will not take too long, you can run the script directly from the terminal:

```sh
python automate_harmonization_pipeline.py --config /path/to/config.ini --verbose
```

##### Running in the Background

If you anticipate that the process will take a long time, you can run the script using the nohup and & commands to run the process in the background:

```sh
nohup python automate_harmonization_pipeline.py --config /path/to/config.ini  &
```

This will run the process in the background and return the control to the terminal. You can check the status of background jobs by using the `jobs` command.

##### Using `screen` for Detached Execution

If you want to run the script in a way that it remains active even after you've closed your terminal session, you can use `screen`:

1. Start a new screen session:

    ```sh
    screen -S harmonization_session
    ```

2. Run the script inside the screen session:

    ```sh
    python automate_harmonization_pipeline.py --config /path/to/config.ini --verbose
    ```

3. Detach from the screen session by pressing `Ctrl + A` followed by `D`.

    To re-attach to the session, use:

    ```sh
    screen -r harmonization_session
    ```

    Here's an example of how to run the script:

    ```sh
    python automate_template_pipeline.py --config /path/to/config.ini --log_file /path/to/logfile.log --verbose
    ```

## Contributing

If you have suggestions for how this script could be improved, please fork this repository and create a pull request, or simply open an issue with the tag "enhancement". Thank you!

## License

This project is licensed under the terms of the MIT license.
